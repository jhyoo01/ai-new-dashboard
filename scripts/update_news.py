#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Updater for GitHub Pages
ì‹¤ì œ AI ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  index.htmlì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from collections import defaultdict
from urllib.parse import quote_plus, urlparse
import hashlib


class AINewsUpdater:
    """AI ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸"""
    
    def __init__(self):
        self.news_data = []
        self.categories = {
            'chatgpt': ['ChatGPT', 'GPT-4', 'GPT-5', 'OpenAI'],
            'gemini': ['Gemini', 'Google AI', 'Bard'],
            'deepseek': ['DeepSeek', 'DeepSeek-V3', 'DeepSeek AI'],
            'qwen': ['Qwen', 'Qwen 2.5', 'Alibaba AI'],
            'kimi': ['Kimi', 'Kimi-K2', 'Moonshot AI']
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.target_sources = ['theverge.com', 'techcrunch.com']  # ì£¼ìš” ì¶œì²˜
    
    def translate_to_korean(self, text):
        """Google Translate APIë¥¼ ì‚¬ìš©í•œ í•œê¸€ ë²ˆì—­"""
        if not text or len(text.strip()) == 0:
            return text
        
        try:
            # Google Translate ë¬´ë£Œ API ì‚¬ìš©
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'en',  # source language: English
                'tl': 'ko',  # target language: Korean
                'dt': 't',
                'q': text
            }
            
            response = requests.get(url, params=params, timeout=5)
            if response.status_code == 200:
                result = response.json()
                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                translated = ''.join([item[0] for item in result[0] if item[0]])
                return translated
            else:
                print(f"    ë²ˆì—­ ì‹¤íŒ¨: {response.status_code}")
                return text
        except Exception as e:
            print(f"    ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text
    
    def search_ai_news(self):
        """ì‹¤ì œ AI ë‰´ìŠ¤ ê²€ìƒ‰"""
        print("ğŸ” ì‹¤ì œ AI ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        # íŠ¹ì • AI ëª¨ë¸ ì¤‘ì‹¬ í‚¤ì›Œë“œ
        keywords = [
            'ChatGPT OpenAI',
            'Google Gemini AI',
            'DeepSeek AI',
            'Qwen Alibaba',
            'Kimi Moonshot AI',
            'ChatGPT news',
            'Gemini update',
            'DeepSeek model',
            'Qwen LLM',
            'Kimi-K2 thinking',
            'OpenAI GPT',
            'Google AI',
            'DeepSeek V3',
            'Qwen 2.5',
            'Moonshot AI'
        ]
        
        for keyword in keywords:
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {keyword}")
                news = self.fetch_real_news(keyword, max_items=10)  # í‚¤ì›Œë“œë‹¹ 10ê°œ
                self.news_data.extend(news)
                time.sleep(1)  # Rate limiting
            except Exception as e:
                print(f"âš ï¸  {keyword} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_news = []
        for item in self.news_data:
            title_hash = hashlib.md5(item['title'].encode()).hexdigest()
            if title_hash not in seen:
                seen.add(title_hash)
                unique_news.append(item)
        
        # ì¤‘ìš”ë„ìˆœ ì •ë ¬
        unique_news.sort(key=lambda x: x['importance'], reverse=True)
        self.news_data = unique_news[:100]  # ìµœëŒ€ 100ê°œ
        
        print(f"âœ… {len(self.news_data)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return self.news_data
    
    def fetch_real_news(self, keyword, max_items=10):
        """ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰ (Google News RSS í™œìš©)"""
        news_items = []
        
        try:
            # Google News RSS í”¼ë“œ ì‚¬ìš©
            rss_url = f"https://news.google.com/rss/search?q={quote_plus(keyword)}&hl=en-US&gl=US&ceid=US:en"
            
            response = requests.get(rss_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return news_items
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')[:max_items * 2]  # í•„í„°ë§ ê³ ë ¤í•´ì„œ 2ë°° ìˆ˜ì§‘
            
            for item in items:
                try:
                    title = item.title.text if item.title else ""
                    link = item.link.text if item.link else ""
                    pub_date = item.pubDate.text if item.pubDate else ""
                    description = item.description.text if item.description else ""
                    source = item.source.text if item.source else "News"
                    
                    # ì¶œì²˜ í•„í„°ë§ (theverge.com, techcrunch.com ìš°ì„ )
                    link_lower = link.lower()
                    is_target_source = any(target in link_lower for target in self.target_sources)
                    
                    # ì„¤ëª…ì—ì„œ HTML íƒœê·¸ ì œê±°
                    description_clean = BeautifulSoup(description, 'html.parser').get_text()
                    description_clean = description_clean[:200] + "..." if len(description_clean) > 200 else description_clean
                    
                    # ë°œí–‰ ì‹œê°„ ê³„ì‚°
                    time_ago = self.calculate_time_ago(pub_date)
                    
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    category = self.classify_category(title + " " + description_clean)
                    
                    # ì¤‘ìš”ë„ ê³„ì‚° (ìš°ì„  ì¶œì²˜ì— ê°€ì¤‘ì¹˜)
                    importance = self.calculate_importance(title, source, is_target_source)
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords_list = self.extract_keywords(title, keyword)
                    
                    # í•œê¸€ ë²ˆì—­
                    print(f"    ë²ˆì—­ ì¤‘: {title[:50]}...")
                    title_ko = self.translate_to_korean(title)
                    description_ko = self.translate_to_korean(description_clean) if description_clean else f"{title_ko}ì— ëŒ€í•œ ìµœì‹  ì†Œì‹ì…ë‹ˆë‹¤."
                    
                    news_items.append({
                        'id': len(self.news_data) + len(news_items) + 1,
                        'title': title_ko,  # í•œê¸€ ì œëª©
                        'source': source,
                        'category': category,
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'time': time_ago,
                        'importance': importance,
                        'description': description_ko,  # í•œê¸€ ì„¤ëª…
                        'link': link,
                        'keywords': keywords_list
                    })
                    
                    # max_items ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                    if len(news_items) >= max_items:
                        break
                    
                    # ë²ˆì—­ API rate limit ë°©ì§€
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"    í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            print(f"  RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        return news_items
    
    def calculate_time_ago(self, pub_date_str):
        """ë°œí–‰ ì‹œê°„ ê³„ì‚°"""
        try:
            from email.utils import parsedate_to_datetime
            pub_date = parsedate_to_datetime(pub_date_str)
            now = datetime.now(pub_date.tzinfo)
            diff = now - pub_date
            
            hours = diff.total_seconds() / 3600
            if hours < 1:
                return f"{int(diff.total_seconds() / 60)}ë¶„ ì „"
            elif hours < 24:
                return f"{int(hours)}ì‹œê°„ ì „"
            else:
                return f"{int(hours / 24)}ì¼ ì „"
        except:
            return "ìµœê·¼"
    
    def calculate_importance(self, title, source, is_target_source=False):
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 7.0
        
        # ìš°ì„  ì¶œì²˜ ê°€ì¤‘ì¹˜ (theverge.com, techcrunch.com)
        if is_target_source:
            score += 1.5
        
        # ì œëª© í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        high_impact = ['breakthrough', 'revolutionary', 'íšê¸°ì ', 'launch', 'ì¶œì‹œ', 'releases', 'unveils']
        medium_impact = ['update', 'ì—…ë°ì´íŠ¸', 'announces', 'ë°œí‘œ', 'reveals']
        
        title_lower = title.lower()
        for word in high_impact:
            if word in title_lower:
                score += 1.5
                break
        for word in medium_impact:
            if word in title_lower:
                score += 0.8
                break
        
        # ì¶œì²˜ ê°€ì¤‘ì¹˜
        premium_sources = ['TechCrunch', 'The Verge', 'MIT', 'Nature', 'Bloomberg', 'Reuters']
        if any(s in source for s in premium_sources):
            score += 1.0
        
        return min(9.5, round(score, 1))
    
    def extract_keywords(self, title, base_keyword):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = [base_keyword.split()[0]]  # ê¸°ë³¸ í‚¤ì›Œë“œ
        
        # ì£¼ìš” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        important_words = ['ChatGPT', 'OpenAI', 'Gemini', 'Google', 'DeepSeek', 
                          'Qwen', 'Kimi', 'Moonshot', 'AI', 'LLM', 'GPT']
        
        for word in important_words:
            if word.lower() in title.lower() and word not in keywords:
                keywords.append(word)
                if len(keywords) >= 3:
                    break
        
        return keywords[:3]
    
    def classify_category(self, text):
        """í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scores = defaultdict(int)
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    scores[category] += 1
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        
        return 'chatgpt'  # ê¸°ë³¸ê°’
    
    def generate_html(self):
        """ì—…ë°ì´íŠ¸ëœ HTML ìƒì„±"""
        print("ğŸ“ HTML ìƒì„± ì¤‘...")
        
        if not self.news_data:
            print("âš ï¸  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.news_data = self.get_default_news()
        
        # JavaScript ë°ì´í„° ë¶€ë¶„ ìƒì„±
        news_json = json.dumps(self.news_data, ensure_ascii=False, indent=8)
        
        # ê¸°ì¡´ HTML í…œí”Œë¦¿ ì½ê¸°
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # NEWS_DATA ë¶€ë¶„ êµì²´
        pattern = r'const NEWS_DATA = \[.*?\];'
        replacement = f'const NEWS_DATA = {news_json};'
        
        updated_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
        
        # ë‚ ì§œ ì—…ë°ì´íŠ¸
        current_date = datetime.now().strftime('%Y-%m-%d')
        updated_html = re.sub(
            r"date: '\d{4}-\d{2}-\d{2}'",
            f"date: '{current_date}'",
            updated_html
        )
        
        return updated_html
    
    def get_default_news(self):
        """ê¸°ë³¸ ë‰´ìŠ¤ ë°ì´í„° (ì‹¤íŒ¨ ì‹œ í´ë°±)"""
        return [
            {
                'id': 1,
                'title': 'OpenAI ìµœì‹  AI ëª¨ë¸ ë°œí‘œ',
                'source': 'TechCrunch',
                'category': 'chatgpt',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '2ì‹œê°„ ì „',
                'importance': 9.5,
                'description': 'OpenAIê°€ ìµœì‹  AI ëª¨ë¸ì„ ê³µê°œí•˜ë©° ì—…ê³„ì— ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=OpenAI+latest+news',
                'keywords': ['OpenAI', 'AI', 'ChatGPT']
            },
            {
                'id': 2,
                'title': 'Google Gemini ì£¼ìš” ì—…ë°ì´íŠ¸',
                'source': 'The Verge',
                'category': 'gemini',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '4ì‹œê°„ ì „',
                'importance': 8.8,
                'description': 'Googleì´ Geminiì˜ ëŒ€ê·œëª¨ ì—…ë°ì´íŠ¸ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=Google+Gemini+update',
                'keywords': ['Google', 'Gemini', 'AI']
            },
            {
                'id': 3,
                'title': 'DeepSeek ìƒˆë¡œìš´ ëª¨ë¸ ì¶œì‹œ',
                'source': 'Ars Technica',
                'category': 'deepseek',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '6ì‹œê°„ ì „',
                'importance': 8.5,
                'description': 'DeepSeekì´ í˜ì‹ ì ì¸ ìƒˆ ëª¨ë¸ì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=DeepSeek+AI+model',
                'keywords': ['DeepSeek', 'AI', 'LLM']
            }
        ]
    
    def save_html(self, html_content):
        """HTML íŒŒì¼ ì €ì¥"""
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        print("âœ… index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ AI News Updater ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            self.search_ai_news()
            
            # 2. HTML ìƒì„±
            html_content = self.generate_html()
            
            # 3. ì €ì¥
            self.save_html(html_content)
            
            print("=" * 60)
            print("âœ¨ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“Š ì´ {len(self.news_data)}ê°œ ê¸°ì‚¬")
            
            # ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤:")
            for i, news in enumerate(self.news_data[:5], 1):
                print(f"{i}. [{news['source']}] {news['title'][:60]}...")
            
            return 0
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    updater = AINewsUpdater()
    return updater.run()


if __name__ == "__main__":
    exit(main())
