#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Updater for GitHub Pages
ì‹¤ì œ AI ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  index.htmlì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from collections import defaultdict
from urllib.parse import quote_plus, urlparse
import hashlib


class AINewsUpdater:
    """AI ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸"""
    
    def __init__(self):
        self.news_data = []
        self.categories = {
            'llm': ['ChatGPT', 'GPT', 'Claude', 'Gemini', 'LLM', 'OpenAI', 'Anthropic', 'ëŒ€í˜•ì–¸ì–´ëª¨ë¸', 'Qwen', 'DeepSeek'],
            'industry': ['AI íˆ¬ì', 'AI ìŠ¤íƒ€íŠ¸ì—…', 'ì‚¼ì„±', 'LG', 'NVIDIA', 'Google', 'AI ê¸°ì—…', 'ê·œì œ', 'AIë²•'],
            'research': ['AI ì—°êµ¬', 'ë…¼ë¬¸', 'ì•Œê³ ë¦¬ì¦˜', 'MIT', 'Stanford', 'ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬'],
            'ml_dl': ['ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§', 'Machine Learning', 'Deep Learning', 'í•™ìŠµ'],
            'application': ['AI í™œìš©', 'AI ì„œë¹„ìŠ¤', 'ì‹ ì•½', 'ì˜ë£Œ', 'ììœ¨ì£¼í–‰', 'ê³ ê°ì„œë¹„ìŠ¤']
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def search_ai_news(self):
        """ì‹¤ì œ AI ë‰´ìŠ¤ ê²€ìƒ‰"""
        print("ğŸ” ì‹¤ì œ AI ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        keywords = [
            'OpenAI latest news',
            'Claude AI Anthropic',
            'Google Gemini AI',
            'DeepSeek AI model',
            'AI regulation news',
            'ChatGPT updates',
            'AI startup funding',
            'AI research breakthrough',
            'NVIDIA AI chip',
            'LLM artificial intelligence'
        ]
        
        for keyword in keywords:
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {keyword}")
                news = self.fetch_real_news(keyword)
                self.news_data.extend(news)
                time.sleep(1)  # Rate limiting
            except Exception as e:
                print(f"âš ï¸  {keyword} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_news = []
        for item in self.news_data:
            title_hash = hashlib.md5(item['title'].encode()).hexdigest()
            if title_hash not in seen:
                seen.add(title_hash)
                unique_news.append(item)
        
        # ì¤‘ìš”ë„ìˆœ ì •ë ¬
        unique_news.sort(key=lambda x: x['importance'], reverse=True)
        self.news_data = unique_news[:12]  # ìµœëŒ€ 12ê°œ
        
        print(f"âœ… {len(self.news_data)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return self.news_data
    
    def fetch_real_news(self, keyword):
        """ì‹¤ì œ ë‰´ìŠ¤ ê²€ìƒ‰ (Google News RSS í™œìš©)"""
        news_items = []
        
        try:
            # Google News RSS í”¼ë“œ ì‚¬ìš©
            rss_url = f"https://news.google.com/rss/search?q={quote_plus(keyword)}&hl=en-US&gl=US&ceid=US:en"
            
            response = requests.get(rss_url, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return news_items
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')[:2]  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 2ê°œ
            
            for item in items:
                try:
                    title = item.title.text if item.title else ""
                    link = item.link.text if item.link else ""
                    pub_date = item.pubDate.text if item.pubDate else ""
                    description = item.description.text if item.description else ""
                    source = item.source.text if item.source else "News"
                    
                    # ì„¤ëª…ì—ì„œ HTML íƒœê·¸ ì œê±°
                    description_clean = BeautifulSoup(description, 'html.parser').get_text()
                    description_clean = description_clean[:200] + "..." if len(description_clean) > 200 else description_clean
                    
                    # ë°œí–‰ ì‹œê°„ ê³„ì‚°
                    time_ago = self.calculate_time_ago(pub_date)
                    
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    category = self.classify_category(title + " " + description_clean)
                    
                    # ì¤‘ìš”ë„ ê³„ì‚°
                    importance = self.calculate_importance(title, source)
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords_list = self.extract_keywords(title, keyword)
                    
                    news_items.append({
                        'id': len(self.news_data) + len(news_items) + 1,
                        'title': title,
                        'source': source,
                        'category': category,
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'time': time_ago,
                        'importance': importance,
                        'description': description_clean or f"{title}ì— ëŒ€í•œ ìµœì‹  ì†Œì‹ì…ë‹ˆë‹¤.",
                        'link': link,
                        'keywords': keywords_list
                    })
                    
                except Exception as e:
                    print(f"    í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            print(f"  RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        return news_items
    
    def calculate_time_ago(self, pub_date_str):
        """ë°œí–‰ ì‹œê°„ ê³„ì‚°"""
        try:
            from email.utils import parsedate_to_datetime
            pub_date = parsedate_to_datetime(pub_date_str)
            now = datetime.now(pub_date.tzinfo)
            diff = now - pub_date
            
            hours = diff.total_seconds() / 3600
            if hours < 1:
                return f"{int(diff.total_seconds() / 60)}ë¶„ ì „"
            elif hours < 24:
                return f"{int(hours)}ì‹œê°„ ì „"
            else:
                return f"{int(hours / 24)}ì¼ ì „"
        except:
            return "ìµœê·¼"
    
    def calculate_importance(self, title, source):
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 7.0
        
        # ì œëª© í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        high_impact = ['breakthrough', 'revolutionary', 'íšê¸°ì ', 'launch', 'ì¶œì‹œ', 'releases', 'unveils']
        medium_impact = ['update', 'ì—…ë°ì´íŠ¸', 'announces', 'ë°œí‘œ', 'reveals']
        
        title_lower = title.lower()
        for word in high_impact:
            if word in title_lower:
                score += 1.5
                break
        for word in medium_impact:
            if word in title_lower:
                score += 0.8
                break
        
        # ì¶œì²˜ ê°€ì¤‘ì¹˜
        premium_sources = ['TechCrunch', 'The Verge', 'MIT', 'Nature', 'Bloomberg', 'Reuters']
        if any(s in source for s in premium_sources):
            score += 1.0
        
        return min(9.5, round(score, 1))
    
    def extract_keywords(self, title, base_keyword):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = [base_keyword.split()[0]]  # ê¸°ë³¸ í‚¤ì›Œë“œ
        
        # ì£¼ìš” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        important_words = ['OpenAI', 'ChatGPT', 'Claude', 'Anthropic', 'Google', 'Gemini', 
                          'DeepSeek', 'AI', 'LLM', 'NVIDIA', 'GPT', 'Qwen']
        
        for word in important_words:
            if word.lower() in title.lower() and word not in keywords:
                keywords.append(word)
                if len(keywords) >= 3:
                    break
        
        return keywords[:3]
    
    def classify_category(self, text):
        """í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text_lower = text.lower()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scores = defaultdict(int)
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    scores[category] += 1
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        
        return 'llm'  # ê¸°ë³¸ê°’
    
    def generate_html(self):
        """ì—…ë°ì´íŠ¸ëœ HTML ìƒì„±"""
        print("ğŸ“ HTML ìƒì„± ì¤‘...")
        
        if not self.news_data:
            print("âš ï¸  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.news_data = self.get_default_news()
        
        # JavaScript ë°ì´í„° ë¶€ë¶„ ìƒì„±
        news_json = json.dumps(self.news_data, ensure_ascii=False, indent=8)
        
        # ê¸°ì¡´ HTML í…œí”Œë¦¿ ì½ê¸°
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # NEWS_DATA ë¶€ë¶„ êµì²´
        pattern = r'const NEWS_DATA = \[.*?\];'
        replacement = f'const NEWS_DATA = {news_json};'
        
        updated_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
        
        # ë‚ ì§œ ì—…ë°ì´íŠ¸
        current_date = datetime.now().strftime('%Y-%m-%d')
        updated_html = re.sub(
            r"date: '\d{4}-\d{2}-\d{2}'",
            f"date: '{current_date}'",
            updated_html
        )
        
        return updated_html
    
    def get_default_news(self):
        """ê¸°ë³¸ ë‰´ìŠ¤ ë°ì´í„° (ì‹¤íŒ¨ ì‹œ í´ë°±)"""
        return [
            {
                'id': 1,
                'title': 'OpenAI ìµœì‹  AI ëª¨ë¸ ë°œí‘œ',
                'source': 'TechCrunch',
                'category': 'llm',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '2ì‹œê°„ ì „',
                'importance': 9.5,
                'description': 'OpenAIê°€ ìµœì‹  AI ëª¨ë¸ì„ ê³µê°œí•˜ë©° ì—…ê³„ì— ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=OpenAI+latest+news',
                'keywords': ['OpenAI', 'AI', 'LLM']
            },
            {
                'id': 2,
                'title': 'Google Gemini ì£¼ìš” ì—…ë°ì´íŠ¸',
                'source': 'The Verge',
                'category': 'llm',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '4ì‹œê°„ ì „',
                'importance': 8.8,
                'description': 'Googleì´ Geminiì˜ ëŒ€ê·œëª¨ ì—…ë°ì´íŠ¸ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=Google+Gemini+update',
                'keywords': ['Google', 'Gemini', 'AI']
            },
            {
                'id': 3,
                'title': 'Anthropic Claude ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€',
                'source': 'Ars Technica',
                'category': 'llm',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'time': '6ì‹œê°„ ì „',
                'importance': 8.5,
                'description': 'Anthropicì´ Claudeì— í˜ì‹ ì ì¸ ìƒˆ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.',
                'link': 'https://www.google.com/search?q=Anthropic+Claude+update',
                'keywords': ['Claude', 'Anthropic', 'AI']
            }
        ]
    
    def save_html(self, html_content):
        """HTML íŒŒì¼ ì €ì¥"""
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        print("âœ… index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ AI News Updater ì‹œì‘...")
        print("=" * 60)
        
        try:
            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            self.search_ai_news()
            
            # 2. HTML ìƒì„±
            html_content = self.generate_html()
            
            # 3. ì €ì¥
            self.save_html(html_content)
            
            print("=" * 60)
            print("âœ¨ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“Š ì´ {len(self.news_data)}ê°œ ê¸°ì‚¬")
            
            # ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤:")
            for i, news in enumerate(self.news_data[:5], 1):
                print(f"{i}. [{news['source']}] {news['title'][:60]}...")
            
            return 0
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    updater = AINewsUpdater()
    return updater.run()


if __name__ == "__main__":
    exit(main())
